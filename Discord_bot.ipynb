{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discord_bot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45c16a7101354aa5ad379a9a35b22d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_179731bf16484c4586042b1bebf0b616",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0465533669874b7683c84bc8bfc5c27d",
              "IPY_MODEL_269c8979d0fe4a0daf7bcd8bc4a4556b",
              "IPY_MODEL_e9423782f76b499886f71384dc83022b"
            ]
          }
        },
        "179731bf16484c4586042b1bebf0b616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0465533669874b7683c84bc8bfc5c27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab72d0d8443b45e0add44bb7442863bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch:  25%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d58b8851b0742a28ff3471e4237dd86"
          }
        },
        "269c8979d0fe4a0daf7bcd8bc4a4556b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f28bddabc46481783269dd92548f4a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6c5ee5ed96c474cb0aa6d72dbc24bc4"
          }
        },
        "e9423782f76b499886f71384dc83022b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d22c7eb0456b491da0ece3d88f51dc1f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/4 [46:43&lt;1:18:07, 1562.54s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33361a94d7994e88957cf7f994e6b1aa"
          }
        },
        "ab72d0d8443b45e0add44bb7442863bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d58b8851b0742a28ff3471e4237dd86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f28bddabc46481783269dd92548f4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6c5ee5ed96c474cb0aa6d72dbc24bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d22c7eb0456b491da0ece3d88f51dc1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33361a94d7994e88957cf7f994e6b1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "498f93530736489daf768fb06bd26303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_239239f437ff40a8b3d9c90b2f7fbdae",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2567a1266a74670b41ddb0c686dbdcd",
              "IPY_MODEL_1b70ca08d17d4192b0d90434dc80306b",
              "IPY_MODEL_8c7b5a6952e248cb8c10ec4b0d70e2fd"
            ]
          }
        },
        "239239f437ff40a8b3d9c90b2f7fbdae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2567a1266a74670b41ddb0c686dbdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_065653171cc94692bf0fa411eeb4693c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Iteration: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af46bf59e277444c9bbf242d3e7c6221"
          }
        },
        "1b70ca08d17d4192b0d90434dc80306b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ea76ae5106354a7d829184810a2f34d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2295,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2295,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9830f3359e744f37bde53792289e069e"
          }
        },
        "8c7b5a6952e248cb8c10ec4b0d70e2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d6b0e05456b45b5998c852857863536",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2295/2295 [26:02&lt;00:00,  1.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0dadbe3ca844f72a42acb1316cf3c1a"
          }
        },
        "065653171cc94692bf0fa411eeb4693c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af46bf59e277444c9bbf242d3e7c6221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea76ae5106354a7d829184810a2f34d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9830f3359e744f37bde53792289e069e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d6b0e05456b45b5998c852857863536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0dadbe3ca844f72a42acb1316cf3c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0330df8b1d734db69687b11a8d6d826a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5196ead78d34afa9ba21c02cbca43c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52b5aa6bb3a84a39b2c70ecfc47ceb0c",
              "IPY_MODEL_16e617660f5042628d0939df7f446f07",
              "IPY_MODEL_f238838a872e480f90563c7b0fc5918c"
            ]
          }
        },
        "f5196ead78d34afa9ba21c02cbca43c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52b5aa6bb3a84a39b2c70ecfc47ceb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9bae5625886e40acb603f0c8766212e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Iteration:  79%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e693c90fe8c24fbc8a571000dffdfe56"
          }
        },
        "16e617660f5042628d0939df7f446f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e14d8012eb2341249790768b7f7f8332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2295,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1818,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_395b5a658df24c7fbcd6704fcbdd0bed"
          }
        },
        "f238838a872e480f90563c7b0fc5918c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c8be25a69bf49d2b27f2c989426ebcf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1818/2295 [20:40&lt;05:27,  1.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d791529dd60d4d38a7c096e6649e3aa2"
          }
        },
        "9bae5625886e40acb603f0c8766212e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e693c90fe8c24fbc8a571000dffdfe56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e14d8012eb2341249790768b7f7f8332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "395b5a658df24c7fbcd6704fcbdd0bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c8be25a69bf49d2b27f2c989426ebcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d791529dd60d4d38a7c096e6649e3aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWAEvyGXK6q",
        "outputId": "348acbf3-d2e4-4395-c09c-0118ce537874"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQo2jOSZt-9"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCVIRv1aDu_"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/1_10_seasons_tbbt.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl4_kqgzaP1N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "32fb7b89-4d4a-42df-c052-fa180bb50a19"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_name</th>\n",
              "      <th>dialogue</th>\n",
              "      <th>person_scene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
              "      <td>A corridor at a sperm bank.</td>\n",
              "      <td>Scene</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
              "      <td>So if a photon is directed through a plane wi...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
              "      <td>Agreed, what’s your point?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
              "      <td>There’s no point, I just think it’s a good id...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
              "      <td>Excuse me?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           episode_name  ... person_scene\n",
              "0  Series 01 Episode 01 – Pilot Episode  ...        Scene\n",
              "1  Series 01 Episode 01 – Pilot Episode  ...      Sheldon\n",
              "2  Series 01 Episode 01 – Pilot Episode  ...      Leonard\n",
              "3  Series 01 Episode 01 – Pilot Episode  ...      Sheldon\n",
              "4  Series 01 Episode 01 – Pilot Episode  ...      Leonard\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxilTv7YasAJ",
        "outputId": "f6d255ce-5dbb-4fea-fda2-91b5a6c5d365"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 54406 entries, 0 to 54405\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   episode_name  54406 non-null  object\n",
            " 1   dialogue      54404 non-null  object\n",
            " 2   person_scene  54406 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hboLaqWea5li"
      },
      "source": [
        "df2 = df.drop(['episode_name'], axis = 1 ,inplace=True)\n",
        "df2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "fO4RtVzsbC0M",
        "outputId": "2e297505-a81b-43ed-ba94-0b51e7572291"
      },
      "source": [
        "df2.reset_index(inplace=True)                                              "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d7879448c792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset_index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "jMIr3PmGcLej",
        "outputId": "ec3b126d-5c07-4a3e-867e-19d66938ca0f"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-68302b2f4bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "jh5WB9hicN1H",
        "outputId": "31bedc20-32d0-4746-8537-b7da39715559"
      },
      "source": [
        "df2.drop(['level_0','index'], axis =1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogue</th>\n",
              "      <th>person_scene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A corridor at a sperm bank.</td>\n",
              "      <td>Scene</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So if a photon is directed through a plane wi...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Agreed, what’s your point?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There’s no point, I just think it’s a good id...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Excuse me?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54401</th>\n",
              "      <td>And I with you. Question, are you seeking a r...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54402</th>\n",
              "      <td>What if I were?</td>\n",
              "      <td>Ramona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54403</th>\n",
              "      <td>Well, that would raise a number of problems. ...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54404</th>\n",
              "      <td>Princeton.</td>\n",
              "      <td>Scene</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54405</th>\n",
              "      <td>(Knock, knock, knock) Amy. (Knock, knock, kno...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54406 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                dialogue person_scene\n",
              "0                            A corridor at a sperm bank.        Scene\n",
              "1       So if a photon is directed through a plane wi...      Sheldon\n",
              "2                             Agreed, what’s your point?      Leonard\n",
              "3       There’s no point, I just think it’s a good id...      Sheldon\n",
              "4                                             Excuse me?      Leonard\n",
              "...                                                  ...          ...\n",
              "54401   And I with you. Question, are you seeking a r...      Sheldon\n",
              "54402                                    What if I were?       Ramona\n",
              "54403   Well, that would raise a number of problems. ...      Sheldon\n",
              "54404                                        Princeton.         Scene\n",
              "54405   (Knock, knock, knock) Amy. (Knock, knock, kno...      Sheldon\n",
              "\n",
              "[54406 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "er9nKAgYivqa",
        "outputId": "fb434ab3-6674-4ad7-c03d-f7fdbba0a7b8"
      },
      "source": [
        "df.drop(index=df[df['person_scene'] == 'Scene'].index, inplace=True)\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogue</th>\n",
              "      <th>person_scene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So if a photon is directed through a plane wi...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Agreed, what’s your point?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There’s no point, I just think it’s a good id...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Excuse me?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hang on.</td>\n",
              "      <td>Receptionist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54400</th>\n",
              "      <td>Mmm. No big deal, I enjoy spending time with ...</td>\n",
              "      <td>Ramona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54401</th>\n",
              "      <td>And I with you. Question, are you seeking a r...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54402</th>\n",
              "      <td>What if I were?</td>\n",
              "      <td>Ramona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54403</th>\n",
              "      <td>Well, that would raise a number of problems. ...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54405</th>\n",
              "      <td>(Knock, knock, knock) Amy. (Knock, knock, kno...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51556 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                dialogue  person_scene\n",
              "1       So if a photon is directed through a plane wi...       Sheldon\n",
              "2                             Agreed, what’s your point?       Leonard\n",
              "3       There’s no point, I just think it’s a good id...       Sheldon\n",
              "4                                             Excuse me?       Leonard\n",
              "5                                              Hang on.   Receptionist\n",
              "...                                                  ...           ...\n",
              "54400   Mmm. No big deal, I enjoy spending time with ...        Ramona\n",
              "54401   And I with you. Question, are you seeking a r...       Sheldon\n",
              "54402                                    What if I were?        Ramona\n",
              "54403   Well, that would raise a number of problems. ...       Sheldon\n",
              "54405   (Knock, knock, knock) Amy. (Knock, knock, kno...       Sheldon\n",
              "\n",
              "[51556 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSrsO6E2iwGA"
      },
      "source": [
        "abc = []\n",
        "abc = df['person_scene'].tolist()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btRtkZ5Mlx4f",
        "outputId": "2de921a9-ede6-4b92-ef44-06628e70fec8"
      },
      "source": [
        "new_abc = set(abc)\n",
        "new_abc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(Wyatt)',\n",
              " '(above)',\n",
              " '(acting)',\n",
              " '(answering)',\n",
              " '(approaching)',\n",
              " '(arriving)',\n",
              " '(belching)',\n",
              " '(burping)',\n",
              " '(chanting)',\n",
              " '(choking)',\n",
              " '(crying)',\n",
              " '(dramatically)',\n",
              " '(entering)',\n",
              " '(enters)',\n",
              " '(excitedly)',\n",
              " '(inside)',\n",
              " '(internally)',\n",
              " '(knocking)',\n",
              " '(laughing)',\n",
              " '(laughs)',\n",
              " '(leaving)',\n",
              " '(lifting)',\n",
              " '(likewise)',\n",
              " '(measures)',\n",
              " '(mouths)',\n",
              " '(off)',\n",
              " '(outside)',\n",
              " '(pff)',\n",
              " '(pointing)',\n",
              " '(rapping)',\n",
              " '(reading)',\n",
              " '(repeatedly)',\n",
              " '(returning)',\n",
              " '(sarcastically)',\n",
              " '(screaming)',\n",
              " '(screams)',\n",
              " '(shouting)',\n",
              " '(signing)',\n",
              " '(simultaneously)',\n",
              " '(singing)',\n",
              " '(sings)',\n",
              " '(snarkily)',\n",
              " '(sniggering)',\n",
              " '(squealing)',\n",
              " '(squeals)',\n",
              " '(together)',\n",
              " '(translating)',\n",
              " '(uncomfortable)',\n",
              " '(undressing)',\n",
              " '(voice)',\n",
              " '(voiceover)',\n",
              " '(whispering)',\n",
              " '(whispers)',\n",
              " '(writing)',\n",
              " '1',\n",
              " '12',\n",
              " '2',\n",
              " 'Abby',\n",
              " 'Actor',\n",
              " 'Actress',\n",
              " 'Adam',\n",
              " 'Alex',\n",
              " 'Alex)',\n",
              " 'Alfred',\n",
              " 'Alice',\n",
              " 'Alicia',\n",
              " 'Alien',\n",
              " 'All',\n",
              " 'Amelia',\n",
              " 'Amy',\n",
              " 'Amy(off)',\n",
              " 'Angela',\n",
              " 'Announcement',\n",
              " 'Announcer',\n",
              " 'Answerphone',\n",
              " 'Apartment',\n",
              " 'Arthur',\n",
              " 'Assistant',\n",
              " 'Attorney',\n",
              " 'Audiitoner',\n",
              " 'Barber',\n",
              " 'Barman',\n",
              " 'Barry',\n",
              " 'Bermadette',\n",
              " 'Bernadette',\n",
              " 'Bert',\n",
              " 'Bethany',\n",
              " 'Beverley',\n",
              " 'Beverly',\n",
              " 'Bill',\n",
              " 'Blanche)',\n",
              " 'Borg)',\n",
              " 'Both',\n",
              " 'Boys',\n",
              " 'Brent',\n",
              " 'Burton',\n",
              " 'Caller',\n",
              " 'Canyon)',\n",
              " 'Caption',\n",
              " 'Child',\n",
              " 'Christie',\n",
              " 'Claire',\n",
              " 'Cleaner',\n",
              " 'Clerk',\n",
              " 'Clock',\n",
              " 'Col',\n",
              " 'Cole',\n",
              " 'Cole)',\n",
              " 'Control',\n",
              " 'Cooper',\n",
              " 'Cooper)',\n",
              " 'Cosmonaut',\n",
              " 'Costume',\n",
              " 'Crawley',\n",
              " 'Crowd',\n",
              " 'Dad',\n",
              " 'Dale',\n",
              " 'Dan',\n",
              " 'Daniel',\n",
              " 'Dave',\n",
              " 'David',\n",
              " 'Davis',\n",
              " 'Dennis',\n",
              " 'Dimitri',\n",
              " 'Director',\n",
              " 'Doctor',\n",
              " 'Doll)',\n",
              " 'Door',\n",
              " 'Doug',\n",
              " 'Driver',\n",
              " 'Elizabeth',\n",
              " 'Ellen',\n",
              " 'Elon',\n",
              " 'Emily',\n",
              " 'Emily)',\n",
              " 'Emly',\n",
              " 'Employee',\n",
              " 'English)',\n",
              " 'Eric',\n",
              " 'Everybody',\n",
              " 'Everyone',\n",
              " 'Factory)',\n",
              " 'Fillion',\n",
              " 'Fisher',\n",
              " 'Flash',\n",
              " 'Flash)',\n",
              " 'Flatow',\n",
              " 'Flowers',\n",
              " 'Fowler',\n",
              " 'GPS',\n",
              " 'Gablehauser',\n",
              " 'Gablehouser',\n",
              " 'Gablehouser)',\n",
              " 'Gallo',\n",
              " 'Gentleman',\n",
              " 'Girl',\n",
              " 'Girls',\n",
              " 'Glau)',\n",
              " 'Glenn',\n",
              " 'God)',\n",
              " 'Goldfarb',\n",
              " 'Greene',\n",
              " 'Gretchen',\n",
              " 'Guard',\n",
              " 'Gunderson',\n",
              " 'Guy',\n",
              " 'Halley',\n",
              " 'Halls)',\n",
              " 'Hawking',\n",
              " 'Headmaster',\n",
              " 'Hernandez',\n",
              " 'Hofstadter',\n",
              " 'Houston',\n",
              " 'Howard',\n",
              " 'Howard)',\n",
              " 'Howatd',\n",
              " 'Inside',\n",
              " 'Ira',\n",
              " 'Isabella',\n",
              " 'James',\n",
              " 'Janine',\n",
              " 'Janitor',\n",
              " 'Jeannie',\n",
              " 'Jeff',\n",
              " 'Jenga)',\n",
              " 'Jenson',\n",
              " 'Jesse',\n",
              " 'Jimmy',\n",
              " 'Jones',\n",
              " 'Josh',\n",
              " 'Joy',\n",
              " 'Judge',\n",
              " 'Katee',\n",
              " 'Kathy',\n",
              " 'Kevin',\n",
              " 'Kevin)',\n",
              " 'Kevn',\n",
              " 'Kid',\n",
              " 'Kim',\n",
              " 'Koothrapalli',\n",
              " 'Koothrappali',\n",
              " 'Korean',\n",
              " 'Kripke',\n",
              " 'Kurt',\n",
              " 'Lady',\n",
              " 'Lakshmi',\n",
              " 'Lalita',\n",
              " 'Latham',\n",
              " 'Laughlin',\n",
              " 'Laura',\n",
              " 'LeVar',\n",
              " 'Lee',\n",
              " 'Leoanard',\n",
              " 'Leonard',\n",
              " 'Leonard)',\n",
              " 'Leonard-warrior',\n",
              " 'Leonard:',\n",
              " 'Lesley',\n",
              " 'Leslie',\n",
              " 'Lorvis',\n",
              " 'Lucy',\n",
              " 'Lucy)',\n",
              " 'M',\n",
              " 'Man',\n",
              " 'Manager',\n",
              " 'Mandarin)',\n",
              " 'Mandel',\n",
              " 'Mandy',\n",
              " 'Martha',\n",
              " 'Marty',\n",
              " 'Mary',\n",
              " 'Mass',\n",
              " 'Massimino',\n",
              " 'Meemaw',\n",
              " 'Michaela',\n",
              " 'Midwife',\n",
              " 'Mike',\n",
              " 'Millstone',\n",
              " 'Minister',\n",
              " 'Missy',\n",
              " 'Mitch)',\n",
              " 'Model',\n",
              " 'Mother',\n",
              " 'Mrs',\n",
              " 'Nathan',\n",
              " 'Nowitzki',\n",
              " 'Nurse',\n",
              " 'Nye',\n",
              " 'Officer',\n",
              " 'Official',\n",
              " 'Outside',\n",
              " 'Page',\n",
              " 'Paramedic',\n",
              " 'Penny',\n",
              " 'Penny(leaving)',\n",
              " 'Penny(voice)',\n",
              " 'Penny)',\n",
              " 'Penny-warrior',\n",
              " 'Petrescu',\n",
              " 'Phone',\n",
              " 'Photographer',\n",
              " 'Picard)',\n",
              " 'Policeman',\n",
              " 'Priya',\n",
              " 'Priya)',\n",
              " 'Psychic',\n",
              " 'R',\n",
              " 'Ra',\n",
              " 'Radio',\n",
              " 'Rai',\n",
              " 'Raj',\n",
              " 'Raj)',\n",
              " 'Rajj',\n",
              " 'Ramona',\n",
              " 'Randall',\n",
              " 'Rebecca',\n",
              " 'Receptionist',\n",
              " 'Registrar',\n",
              " 'Ricky)',\n",
              " 'Roeger',\n",
              " 'Rostenkowski',\n",
              " 'Rothman',\n",
              " 'Russian)',\n",
              " 'Sackhoff',\n",
              " 'Santa',\n",
              " 'Sarah',\n",
              " 'Scanner',\n",
              " 'Sceme',\n",
              " 'Schneider',\n",
              " 'Screen',\n",
              " 'Secne',\n",
              " 'Sehldon',\n",
              " 'Seibert',\n",
              " 'Server',\n",
              " 'Sgeldon',\n",
              " 'Sharpe',\n",
              " 'Shedon',\n",
              " 'Sheldon',\n",
              " 'Sheldon)',\n",
              " 'Sheldon-bot',\n",
              " 'Shldon',\n",
              " 'Shopkeeper',\n",
              " 'Siebert',\n",
              " 'Siri',\n",
              " 'Skype)',\n",
              " 'Smoot',\n",
              " 'Spiner',\n",
              " 'Spock',\n",
              " 'Staff',\n",
              " 'Steph',\n",
              " 'Stephen',\n",
              " 'Story',\n",
              " 'Stuart',\n",
              " 'Suit',\n",
              " 'Summer',\n",
              " 'Susan',\n",
              " 'Sweatpants',\n",
              " 'TV',\n",
              " 'Takei',\n",
              " 'Tardis)',\n",
              " 'Tattooist',\n",
              " 'Teleplay',\n",
              " 'Theodore',\n",
              " 'Thief',\n",
              " 'Thor)',\n",
              " 'Toby',\n",
              " 'Todd',\n",
              " 'Together',\n",
              " 'Tom',\n",
              " 'Transvestite',\n",
              " 'Trent',\n",
              " 'Tyson',\n",
              " 'Vanessa',\n",
              " 'Various',\n",
              " 'Venkatesh',\n",
              " 'Vet',\n",
              " 'Voice',\n",
              " 'Waiter',\n",
              " 'Waitress',\n",
              " 'Warrior',\n",
              " 'West',\n",
              " 'Wheaton',\n",
              " 'Wil',\n",
              " 'Williams',\n",
              " 'Winkle',\n",
              " 'Witch',\n",
              " 'Wolowitz',\n",
              " 'Wolowitz)',\n",
              " 'Woman',\n",
              " 'Wozniak',\n",
              " 'Wyatt',\n",
              " 'Zack',\n",
              " 'Zombie',\n",
              " 'accent)',\n",
              " 'again)',\n",
              " 'alert)',\n",
              " 'angry)',\n",
              " 'answerphone)',\n",
              " 'apartment)',\n",
              " 'approach)',\n",
              " 'assistant',\n",
              " 'back)',\n",
              " 'background)',\n",
              " 'barmaid)',\n",
              " 'bathroom)',\n",
              " 'bed)',\n",
              " 'beer)',\n",
              " 'begins)',\n",
              " 'behind',\n",
              " 'behind)',\n",
              " 'blanket)',\n",
              " 'bleary)',\n",
              " 'box',\n",
              " 'box)',\n",
              " 'brain)',\n",
              " 'breath)',\n",
              " 'brother)',\n",
              " 'buzzer',\n",
              " 'buzzer)',\n",
              " 'cap)',\n",
              " 'captions)',\n",
              " 'car)',\n",
              " 'ceiling)',\n",
              " 'chair)',\n",
              " 'change)',\n",
              " 'child',\n",
              " 'children)',\n",
              " 'chimes)',\n",
              " 'claps)',\n",
              " 'clearance)',\n",
              " 'clerk',\n",
              " 'coat',\n",
              " 'condoms)',\n",
              " 'confusion)',\n",
              " 'cord)',\n",
              " 'costume)',\n",
              " 'coupon)',\n",
              " 'cricket)',\n",
              " 'cupboard)',\n",
              " 'cushion)',\n",
              " 'customer)',\n",
              " 'customers)',\n",
              " 'dancing)',\n",
              " 'daydream)',\n",
              " 'daydreaming)',\n",
              " 'demonstrating)',\n",
              " 'dice)',\n",
              " 'doll)',\n",
              " 'door',\n",
              " 'door)',\n",
              " 'doorway)',\n",
              " 'down)',\n",
              " 'dream)',\n",
              " 'dress)',\n",
              " 'drink)',\n",
              " 'driver',\n",
              " 'duvet)',\n",
              " 'd’',\n",
              " 'each)',\n",
              " 'ear)',\n",
              " 'embarrassed)',\n",
              " 'employee',\n",
              " 'entered)',\n",
              " 'entering)',\n",
              " 'enters)',\n",
              " 'excitedly)',\n",
              " 'face)',\n",
              " 'facetime)',\n",
              " 'father)',\n",
              " 'fit)',\n",
              " 'flashback)',\n",
              " 'flat)',\n",
              " 'floor)',\n",
              " 'gather)',\n",
              " 'gear)',\n",
              " 'genitals)',\n",
              " 'girl',\n",
              " 'glasses)',\n",
              " 'grasshopper)',\n",
              " 'grinning)',\n",
              " 'ground)',\n",
              " 'guy',\n",
              " 'guys',\n",
              " 'hair)',\n",
              " 'hallway)',\n",
              " 'hat',\n",
              " 'head',\n",
              " 'heard)',\n",
              " 'her)',\n",
              " 'him)',\n",
              " 'himself)',\n",
              " 'hug)',\n",
              " 'in)',\n",
              " 'inside)',\n",
              " 'instructor',\n",
              " 'it)',\n",
              " 'kiss)',\n",
              " 'knock)',\n",
              " 'lab)',\n",
              " 'laptop)',\n",
              " 'laugh)',\n",
              " 'laughs)',\n",
              " 'leave)',\n",
              " 'lightsabre)',\n",
              " 'likewise)',\n",
              " 'machine',\n",
              " 'machine)',\n",
              " 'mailbox)',\n",
              " 'man',\n",
              " 'mat)',\n",
              " 'microphone)',\n",
              " 'mini-cam)',\n",
              " 'mirror)',\n",
              " 'mother',\n",
              " 'mother)',\n",
              " 'mouth)',\n",
              " 'moves)',\n",
              " 'naked)',\n",
              " 'napkin)',\n",
              " 'noise)',\n",
              " 'noises)',\n",
              " 'nose)',\n",
              " 'nothing)',\n",
              " 'noticeboard)',\n",
              " 'off)',\n",
              " 'off-screen)',\n",
              " 'on)',\n",
              " 'once)',\n",
              " 'one',\n",
              " 'open)',\n",
              " 'other)',\n",
              " 'outside)',\n",
              " 'oven)',\n",
              " 'over)',\n",
              " 'pane)',\n",
              " 'panic)',\n",
              " 'past)',\n",
              " 'patch)',\n",
              " 'path)',\n",
              " 'patrons',\n",
              " 'pause)',\n",
              " 'phone',\n",
              " 'phone)',\n",
              " 'pitch)',\n",
              " 'plate)',\n",
              " 'pocket.)',\n",
              " 'pregnant)',\n",
              " 'quartettist',\n",
              " 'queue',\n",
              " 'queue)',\n",
              " 'randomly)',\n",
              " 'receiver)',\n",
              " 'rhinestones)',\n",
              " 'ringing)',\n",
              " 'rings)',\n",
              " 'rings.)',\n",
              " 'room)',\n",
              " 'round)',\n",
              " 'rung)',\n",
              " 'running)',\n",
              " 'scene)',\n",
              " 'scenes',\n",
              " 'screaming)',\n",
              " 'screams)',\n",
              " 'screen',\n",
              " 'screen)',\n",
              " 'seat)',\n",
              " 'seats)',\n",
              " 'self)',\n",
              " 'sequence',\n",
              " 'ship)',\n",
              " 'shirt)',\n",
              " 'shoes)',\n",
              " 'shoulder)',\n",
              " 'side)',\n",
              " 'sigh)',\n",
              " 'singing)',\n",
              " 'skype)',\n",
              " 'sleep)',\n",
              " 'smiling)',\n",
              " 'sneeze)',\n",
              " 'sneezes)',\n",
              " 'soaked)',\n",
              " 'sofa)',\n",
              " 'sound)',\n",
              " 'spot)',\n",
              " 'spray)',\n",
              " 'squeak)',\n",
              " 'staff',\n",
              " 'stairs',\n",
              " 'stairs)',\n",
              " 'statue)',\n",
              " 'sticker)',\n",
              " 'subtitles)',\n",
              " 'suddenly)',\n",
              " 'suitcase)',\n",
              " 'supplements)',\n",
              " 'support)',\n",
              " 'table',\n",
              " 'tablet)',\n",
              " 'talk)',\n",
              " 'talking)',\n",
              " 'tears)',\n",
              " 'teeth)',\n",
              " 'television',\n",
              " 'television)',\n",
              " 'tequila)',\n",
              " 'text)',\n",
              " 'them)',\n",
              " 'thought)',\n",
              " 'three',\n",
              " 'time)',\n",
              " 'times)',\n",
              " 'together',\n",
              " 'tone)',\n",
              " 'tow)',\n",
              " 'train)',\n",
              " 'tray)',\n",
              " 'trousers)',\n",
              " 'tune)',\n",
              " 'tunelessly)',\n",
              " 'tuts)',\n",
              " 'twitter)',\n",
              " 'two',\n",
              " 'uncomfortably)',\n",
              " 'undershorts.)',\n",
              " 'unwashed)',\n",
              " 'up)',\n",
              " 'vaporub)',\n",
              " 'video)',\n",
              " 'voice',\n",
              " 'voice)',\n",
              " 'walkie-talkie)',\n",
              " 'warrior',\n",
              " 'way)',\n",
              " 'webcam)',\n",
              " 'whispers)',\n",
              " 'whiteboards)',\n",
              " 'window)',\n",
              " 'wine)',\n",
              " 'within)',\n",
              " 'woman',\n",
              " 'women)',\n",
              " 'workshop)',\n",
              " 'youtube)'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68dlvbmdq2NK"
      },
      "source": [
        "Character =['Amy','Bernadette','Howard','Leonard','Sehldon','Raj','Penny']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64wQSn98dUmB",
        "outputId": "68b334f2-49be-4056-b30e-25042b2aa455"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.8 MB 12.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 25.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 46.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyo9U8CgcYU"
      },
      "source": [
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJZYUegWyXIl"
      },
      "source": [
        "df.rename(columns= {'person_scene': 'name', 'dialogue':'line'}, inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foyDUiZS_OJk"
      },
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "GR-eNw2IAQsO",
        "outputId": "c5a1ae15-18ea-47dd-898a-04e5814b65d1"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>line</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>So if a photon is directed through a plane wi...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Agreed, what’s your point?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>There’s no point, I just think it’s a good id...</td>\n",
              "      <td>Sheldon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Excuse me?</td>\n",
              "      <td>Leonard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Hang on.</td>\n",
              "      <td>Receptionist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                               line          name\n",
              "0      1   So if a photon is directed through a plane wi...       Sheldon\n",
              "1      2                         Agreed, what’s your point?       Leonard\n",
              "2      3   There’s no point, I just think it’s a good id...       Sheldon\n",
              "3      4                                         Excuse me?       Leonard\n",
              "4      5                                          Hang on.   Receptionist"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k4Rbvpd_lcy"
      },
      "source": [
        "CHARACTER_NAME = 'Sheldon'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OYRH4yGgtkW"
      },
      "source": [
        "contexted = []\n",
        "\n",
        "# context window of size 7\n",
        "n = 10\n",
        "\n",
        "for i in df[df.name == CHARACTER_NAME].index:\n",
        "  if i < n:\n",
        "    continue\n",
        "  row = []\n",
        "  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 10 previous responces  \n",
        "  for j in range(i, prev, -1):\n",
        "    row.append(df.line[j])\n",
        "  contexted.append(row)\n",
        "\n",
        "columns = ['response', 'context'] \n",
        "columns = columns + ['context/' + str(i) for i in range(n - 1)]\n",
        "\n",
        "data = pd.DataFrame.from_records(contexted, columns=columns)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygn-MfnnunoM"
      },
      "source": [
        "data = data.dropna()\n",
        "# texts = data[\"texts\"].tolist()\n",
        "# texts_encodings = tokenizer(texts, truncation=True, padding=True)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuZlKfrxAYi4"
      },
      "source": [
        "train_df,val_df = train_test_split(data, test_size=0.2 ,random_state = 42)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "-ULsQ8mVFJoH",
        "outputId": "c18536fc-1912-40d3-a2f3-86160974aad4"
      },
      "source": [
        "train_df.head(5)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "      <th>context/0</th>\n",
              "      <th>context/1</th>\n",
              "      <th>context/2</th>\n",
              "      <th>context/3</th>\n",
              "      <th>context/4</th>\n",
              "      <th>context/5</th>\n",
              "      <th>context/6</th>\n",
              "      <th>context/7</th>\n",
              "      <th>context/8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2388</th>\n",
              "      <td>(Knock, knock, knock) Penny.</td>\n",
              "      <td>Excellent. And just an FYI, as I am the exped...</td>\n",
              "      <td>Oh, damn it. Peer pressure. Fine.</td>\n",
              "      <td>Me, too.</td>\n",
              "      <td>I’m in.</td>\n",
              "      <td>Well, gentlemen, have you reached a decision?</td>\n",
              "      <td>Well, I’m a Hindu. My religion teaches that i...</td>\n",
              "      <td>And you think you can put up with Sheldon?</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>So you guys are seriously considering this?</td>\n",
              "      <td>You still might get on a magazine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8499</th>\n",
              "      <td>Yes. So much so that I started to panic.</td>\n",
              "      <td>You did?</td>\n",
              "      <td>I really did think you looked pretty.</td>\n",
              "      <td>Sheldon, this is silly. I’m not missing anoth...</td>\n",
              "      <td>Not until he stops humping his way up my fami...</td>\n",
              "      <td>Howie, get off of him.</td>\n",
              "      <td>I know, me, too. But it will be fun to have a...</td>\n",
              "      <td>I sent them a bikini shot of you years ago. T...</td>\n",
              "      <td>Want to take a picture of us and send it to y...</td>\n",
              "      <td>Look at me. Dancing with the prettiest girl a...</td>\n",
              "      <td>Thank you for wearing your heels.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3333</th>\n",
              "      <td>Waterfalls!</td>\n",
              "      <td>We’re getting to him.</td>\n",
              "      <td>I said stop it!</td>\n",
              "      <td>Yeah, Meemaw did the nasty.</td>\n",
              "      <td>Stop it!</td>\n",
              "      <td>All right. I’ll bet your Meemaw didn’t just h...</td>\n",
              "      <td>Never.</td>\n",
              "      <td>Then let go of the ring and walk away.</td>\n",
              "      <td>I don’t want to hear this.</td>\n",
              "      <td>Think about this. The only way your mother wa...</td>\n",
              "      <td>No! I call no Meemaws.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>(Knock, knock, knock) Penny.</td>\n",
              "      <td>(Knock, knock, knock) Sheldon.</td>\n",
              "      <td>(Knock, knock, knock) Penny.</td>\n",
              "      <td>Excellent. And just an FYI, as I am the exped...</td>\n",
              "      <td>Oh, damn it. Peer pressure. Fine.</td>\n",
              "      <td>Me, too.</td>\n",
              "      <td>I’m in.</td>\n",
              "      <td>Well, gentlemen, have you reached a decision?</td>\n",
              "      <td>Well, I’m a Hindu. My religion teaches that i...</td>\n",
              "      <td>And you think you can put up with Sheldon?</td>\n",
              "      <td>Yes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10540</th>\n",
              "      <td>Wow. I anticipated we’d have problems, but I ...</td>\n",
              "      <td>The kind who almost put a pillow over your fa...</td>\n",
              "      <td>Well, but what kind of scientists would we be...</td>\n",
              "      <td>Sheldon, maybe living together is a bad idea.</td>\n",
              "      <td>How? You had the whole floor to yourself.</td>\n",
              "      <td>Well, I didn’t, and it’s your fault.</td>\n",
              "      <td>Really? I slept great.</td>\n",
              "      <td>Miserable and exhausted.</td>\n",
              "      <td>Good morning. See? I didn’t knock, but it’s f...</td>\n",
              "      <td>Yeah.</td>\n",
              "      <td>If you ever need a break, the owner of the tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                response  ...                                          context/8\n",
              "2388                        (Knock, knock, knock) Penny.  ...                 You still might get on a magazine.\n",
              "8499            Yes. So much so that I started to panic.  ...                  Thank you for wearing your heels.\n",
              "3333                                         Waterfalls!  ...                             No! I call no Meemaws.\n",
              "2389                        (Knock, knock, knock) Penny.  ...                                               Yes.\n",
              "10540   Wow. I anticipated we’d have problems, but I ...  ...   If you ever need a break, the owner of the tr...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "quRYHvOcFMxo",
        "outputId": "4dd0bf93-ba18-48b1-fb09-22c45e36487f"
      },
      "source": [
        "val_df.head(5)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "      <th>context/0</th>\n",
              "      <th>context/1</th>\n",
              "      <th>context/2</th>\n",
              "      <th>context/3</th>\n",
              "      <th>context/4</th>\n",
              "      <th>context/5</th>\n",
              "      <th>context/6</th>\n",
              "      <th>context/7</th>\n",
              "      <th>context/8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021</th>\n",
              "      <td>But. Oh, this changes everything.</td>\n",
              "      <td>Yeah, well, before they went out of business,...</td>\n",
              "      <td>No. No, this isn’t right. Our food always com...</td>\n",
              "      <td>Golden Dragon.</td>\n",
              "      <td>What? Wh-Where did my cashew chicken come from?</td>\n",
              "      <td>Szechuan Palace closed two years ago.</td>\n",
              "      <td>Yes. From Szechuan Palace.</td>\n",
              "      <td>I’m afraid so. You know the cashew chicken I ...</td>\n",
              "      <td>More?</td>\n",
              "      <td>Penny, Penny, I think I know what to do. Shel...</td>\n",
              "      <td>It’s exactly the same…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9242</th>\n",
              "      <td>That’s right. It’s called foreplay. And I cou...</td>\n",
              "      <td>Yeah, you’ve been going out for years. You ha...</td>\n",
              "      <td>Too slow?</td>\n",
              "      <td>Hey, I don’t think she’s wrong about you goin...</td>\n",
              "      <td>No, and I’m confused. It’s been nearly 24 hou...</td>\n",
              "      <td>Still haven’t heard from her?</td>\n",
              "      <td>Yeah. I wish I’d fought harder for the rest o...</td>\n",
              "      <td>That carrot was delicious.</td>\n",
              "      <td>Mmm. My pleasure.</td>\n",
              "      <td>Thanks for cooking.</td>\n",
              "      <td>That’s not true. We were just laughing right ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10798</th>\n",
              "      <td>I’ll be right across the hall. You’ll probabl...</td>\n",
              "      <td>So you think you should keep it?</td>\n",
              "      <td>Well, Leonard, you know, who should keep this...</td>\n",
              "      <td>So if it breaks there’d be none of it?</td>\n",
              "      <td>It’s one of a kind.</td>\n",
              "      <td>Live long and prosper. Live long and prosper....</td>\n",
              "      <td>Oh, yeah. The Mr. Spock cuckoo clock.</td>\n",
              "      <td>Oh. Remember when we got this at Comic-Con?</td>\n",
              "      <td>Yeah, let him do it.</td>\n",
              "      <td>Howard just did that.</td>\n",
              "      <td>Then I’ll check the batteries in the smoke de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2820</th>\n",
              "      <td>But I could call you a cab or an ambulance.</td>\n",
              "      <td>Ow.</td>\n",
              "      <td>Well, it seems we’ve reached an impasse.</td>\n",
              "      <td>Well, I can’t drive!</td>\n",
              "      <td>I don’t drive.</td>\n",
              "      <td>Okay, can you drive me?</td>\n",
              "      <td>Well, assuming you’re correct that your right...</td>\n",
              "      <td>Oh, my god. I got to go to the emergency room.</td>\n",
              "      <td>They’re whimsical because ducks have neither ...</td>\n",
              "      <td>Uh-huh.</td>\n",
              "      <td>The ducks in my tub.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6001</th>\n",
              "      <td>You’re obviously good at what you do.</td>\n",
              "      <td>No, about my job. I want you to tell me I’m g...</td>\n",
              "      <td>Fine. You have very tiny hands.</td>\n",
              "      <td>Give me a compliment.</td>\n",
              "      <td>Okay.</td>\n",
              "      <td>All right, Sheldon, there’s only one thing le...</td>\n",
              "      <td>Oh, for heaven’s sake. I did your laundry, I ...</td>\n",
              "      <td>I don’t know.</td>\n",
              "      <td>Humorous. Now will you please present my pape...</td>\n",
              "      <td>I should have sent you to the custom car cove...</td>\n",
              "      <td>I did it. Had to go to three clothing stores,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                response  ...                                          context/8\n",
              "2021                   But. Oh, this changes everything.  ...                             It’s exactly the same…\n",
              "9242    That’s right. It’s called foreplay. And I cou...  ...   That’s not true. We were just laughing right ...\n",
              "10798   I’ll be right across the hall. You’ll probabl...  ...   Then I’ll check the batteries in the smoke de...\n",
              "2820         But I could call you a cab or an ambulance.  ...                               The ducks in my tub.\n",
              "6001               You’re obviously good at what you do.  ...   I did it. Had to go to three clothing stores,...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfVxpYzNFPit"
      },
      "source": [
        "##Creating dataset for our model .\n",
        "##Starts here............................."
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T68hNE1If2z"
      },
      "source": [
        "def construct_conv(row,tokenizer, eos = True):\n",
        "  flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "  \n",
        "  conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id]\n",
        "                        for x in row]))\n",
        "  \n",
        "  conv = flatten(conv)\n",
        "\n",
        "  return conv"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6GIa2kzJzlc"
      },
      "source": [
        "\n",
        "# create dataset suitable for our model\n",
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            for _, row in df.iterrows():\n",
        "                conv = construct_conv(row, tokenizer)\n",
        "                self.examples.append(conv)\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7OKXgcGL6Ac"
      },
      "source": [
        "## Ends here .....\n",
        "\n",
        "#Cacheing and sarting of checkpoints\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW4d2_0fMIM5"
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
        "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Mc4GoIMQR8"
      },
      "source": [
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikjEEjmMMS4X"
      },
      "source": [
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Siz84rMWhw"
      },
      "source": [
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vx8tAALMe91"
      },
      "source": [
        "\n",
        "## caching end here..\n",
        "\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evl5W8DrMjoa"
      },
      "source": [
        "\n",
        "#MModel build"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx7YiaN2Mtep",
        "outputId": "0e63c1cd-30df-46b4-c334-91d41cc7c4b3"
      },
      "source": [
        "\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELDuC2tQM0rh"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "# Configs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpzZE8IYOto0"
      },
      "source": [
        "\n",
        "\n",
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = 'output-small'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
        "        self.config_name = 'microsoft/DialoGPT-small'\n",
        "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.do_train = True\n",
        "        self.do_eval = True\n",
        "        self.evaluate_during_training = False\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.per_gpu_eval_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 4\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_steps = 3500\n",
        "        self.save_total_limit = None\n",
        "        self.eval_all_checkpoints = False\n",
        "        self.no_cuda = False\n",
        "        self.overwrite_output_dir = True\n",
        "        self.overwrite_cache = True\n",
        "        self.should_continue = False\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "        self.fp16 = False\n",
        "        self.fp16_opt_level = 'O1'\n",
        "\n",
        "args = Args()"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3IUiaOrOwO9"
      },
      "source": [
        "\n",
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    # add_special_tokens_(model, tokenizer)\n",
        "\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "# Evaluation of some model\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
        "    os.makedirs(eval_output_dir, exist_ok=True)\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txDTMPN3PGmU"
      },
      "source": [
        "\n",
        "\n",
        "# Main runner\n",
        "\n",
        "def main(df_trn, df_val):\n",
        "    args = Args()\n",
        "    \n",
        "    if args.should_continue:\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\n",
        "        if len(sorted_checkpoints) == 0:\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "        else:\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelWithLMHead.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=False,\n",
        "        config=config,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "    model.to(args.device)\n",
        "    \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train:\n",
        "        # Create output directory if needed\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734,
          "referenced_widgets": [
            "45c16a7101354aa5ad379a9a35b22d46",
            "179731bf16484c4586042b1bebf0b616",
            "0465533669874b7683c84bc8bfc5c27d",
            "269c8979d0fe4a0daf7bcd8bc4a4556b",
            "e9423782f76b499886f71384dc83022b",
            "ab72d0d8443b45e0add44bb7442863bd",
            "3d58b8851b0742a28ff3471e4237dd86",
            "1f28bddabc46481783269dd92548f4a6",
            "c6c5ee5ed96c474cb0aa6d72dbc24bc4",
            "d22c7eb0456b491da0ece3d88f51dc1f",
            "33361a94d7994e88957cf7f994e6b1aa",
            "498f93530736489daf768fb06bd26303",
            "239239f437ff40a8b3d9c90b2f7fbdae",
            "f2567a1266a74670b41ddb0c686dbdcd",
            "1b70ca08d17d4192b0d90434dc80306b",
            "8c7b5a6952e248cb8c10ec4b0d70e2fd",
            "065653171cc94692bf0fa411eeb4693c",
            "af46bf59e277444c9bbf242d3e7c6221",
            "ea76ae5106354a7d829184810a2f34d7",
            "9830f3359e744f37bde53792289e069e",
            "9d6b0e05456b45b5998c852857863536",
            "d0dadbe3ca844f72a42acb1316cf3c1a",
            "0330df8b1d734db69687b11a8d6d826a",
            "f5196ead78d34afa9ba21c02cbca43c1",
            "52b5aa6bb3a84a39b2c70ecfc47ceb0c",
            "16e617660f5042628d0939df7f446f07",
            "f238838a872e480f90563c7b0fc5918c",
            "9bae5625886e40acb603f0c8766212e3",
            "e693c90fe8c24fbc8a571000dffdfe56",
            "e14d8012eb2341249790768b7f7f8332",
            "395b5a658df24c7fbcd6704fcbdd0bed",
            "1c8be25a69bf49d2b27f2c989426ebcf",
            "d791529dd60d4d38a7c096e6649e3aa2"
          ]
        },
        "id": "YAEgvlHuYGWa",
        "outputId": "242e9ff4-daa0-41fb-8e3e-8210209fd9fc"
      },
      "source": [
        "main(train_df, val_df)\n"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/04/2021 10:51:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/04/2021 10:51:35 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f30b42e3710>\n",
            "09/04/2021 10:51:35 - INFO - __main__ -   Creating features from dataset file at cached\n",
            "09/04/2021 10:51:56 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
            "09/04/2021 10:51:57 - INFO - __main__ -   ***** Running training *****\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Num examples = 9182\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Num Epochs = 4\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "09/04/2021 10:51:57 - INFO - __main__ -     Total optimization steps = 9180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45c16a7101354aa5ad379a9a35b22d46",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "498f93530736489daf768fb06bd26303",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Iteration:   0%|          | 0/2295 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0330df8b1d734db69687b11a8d6d826a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Iteration:   0%|          | 0/2295 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/04/2021 11:31:37 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-3500\n",
            "09/04/2021 11:31:42 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-3500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-c453db27d3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-147-2724a3197762>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(df_trn, df_val)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-e28c76f66822>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 11.17 GiB total capacity; 9.33 GiB already allocated; 325.81 MiB free; 10.40 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nvpj6cCPP_v",
        "outputId": "c2fe21f3-876f-430f-af96-b5253780b462"
      },
      "source": [
        "sum(train_df['context/4'].isnull())\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1TCmnKTYDN2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "weKDtkYlPTyc",
        "outputId": "5b605843-cc7e-41f5-a05b-4744bc588438"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
        "model = AutoModelWithLMHead.from_pretrained('output-small')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "404 Client Error: Not Found for url: https://huggingface.co/output-small/resolve/main/config.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/output-small/resolve/main/config.json",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-2ab2e1bbd93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'microsoft/DialoGPT-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         )\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m             )\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \"\"\"\n\u001b[1;32m    514\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for 'output-small'. Make sure that:\n\n- 'output-small' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'output-small' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Lz-zmJVOyi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}